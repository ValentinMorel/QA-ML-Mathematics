{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Summary<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Questions-/-Answers-in-Machine-Learning\" data-toc-modified-id=\"Questions-/-Answers-in-Machine-Learning-1\">Questions / Answers in Machine Learning</a></span><ul class=\"toc-item\"><li><span><a href=\"#Part-1-:-Linear-Algebra\" data-toc-modified-id=\"Part-1-:-Linear-Algebra-1.1\">Part 1 : Linear Algebra</a></span><ul class=\"toc-item\"><li><span><a href=\"#1.1-What-is-broadcasting-in-connection-to-Linear-Algebra?\" data-toc-modified-id=\"1.1-What-is-broadcasting-in-connection-to-Linear-Algebra?-1.1.1\">1.1 What is broadcasting in connection to Linear Algebra?</a></span></li><li><span><a href=\"#1.2-What-are-scalars,-vectors,-matrices,-and-tensors?\" data-toc-modified-id=\"1.2-What-are-scalars,-vectors,-matrices,-and-tensors?-1.1.2\">1.2 What are scalars, vectors, matrices, and tensors?</a></span></li><li><span><a href=\"#1.3-What-is-Hadamard-product-of-two-matrices?\" data-toc-modified-id=\"1.3-What-is-Hadamard-product-of-two-matrices?-1.1.3\">1.3 What is Hadamard product of two matrices?</a></span></li><li><span><a href=\"#1.4-What-is-an-inverse-matrix-?\" data-toc-modified-id=\"1.4-What-is-an-inverse-matrix-?-1.1.4\">1.4 What is an inverse matrix ?</a></span></li><li><span><a href=\"#1.5--If-inverse-of-a-matrix-exists,-how-to-calculate-it?\" data-toc-modified-id=\"1.5--If-inverse-of-a-matrix-exists,-how-to-calculate-it?-1.1.5\">1.5  If inverse of a matrix exists, how to calculate it?</a></span></li><li><span><a href=\"#1.6-What-is-the-determinant-of-a-square-matrix?-How-is-it-calculated?-What-is-the-connection-of-determinant-to-eigenvalues?\" data-toc-modified-id=\"1.6-What-is-the-determinant-of-a-square-matrix?-How-is-it-calculated?-What-is-the-connection-of-determinant-to-eigenvalues?-1.1.6\">1.6 What is the determinant of a square matrix? How is it calculated? What is the connection of determinant to eigenvalues?</a></span></li><li><span><a href=\"#1.7-Discuss-span-and-linear-dependence.\" data-toc-modified-id=\"1.7-Discuss-span-and-linear-dependence.-1.1.7\">1.7 Discuss span and linear dependence.</a></span></li><li><span><a href=\"#1.8-What-is-Ax-=-b?-When-does-Ax-=b-has-a-unique-solution?\" data-toc-modified-id=\"1.8-What-is-Ax-=-b?-When-does-Ax-=b-has-a-unique-solution?-1.1.8\">1.8 What is Ax = b? When does Ax =b has a unique solution?</a></span></li><li><span><a href=\"#1.9-In-Ax-=-b,-what-happens-when-A-is-fat-or-tall?\" data-toc-modified-id=\"1.9-In-Ax-=-b,-what-happens-when-A-is-fat-or-tall?-1.1.9\">1.9 In Ax = b, what happens when A is fat or tall?</a></span></li><li><span><a href=\"#1.10-When-does-inverse-of-A-exist?\" data-toc-modified-id=\"1.10-When-does-inverse-of-A-exist?-1.1.10\">1.10 When does inverse of A exist?</a></span></li><li><span><a href=\"#1.11-What-is-a-norm?-What-is-L1,-L2-and-L-infinity-norm?\" data-toc-modified-id=\"1.11-What-is-a-norm?-What-is-L1,-L2-and-L-infinity-norm?-1.1.11\">1.11 What is a norm? What is L1, L2 and L infinity norm?</a></span></li><li><span><a href=\"#1.12-What-are-the-conditions-a-norm-has-to-satisfy?\" data-toc-modified-id=\"1.12-What-are-the-conditions-a-norm-has-to-satisfy?-1.1.12\">1.12 What are the conditions a norm has to satisfy?</a></span></li><li><span><a href=\"#1.13-Why-is-squared-of-L2-norm-preferred-in-ML-than-just-L2-norm?\" data-toc-modified-id=\"1.13-Why-is-squared-of-L2-norm-preferred-in-ML-than-just-L2-norm?-1.1.13\">1.13 Why is squared of L2 norm preferred in ML than just L2 norm?</a></span></li><li><span><a href=\"#1.14-When-L1-norm-is-preferred-over-L2-norm?\" data-toc-modified-id=\"1.14-When-L1-norm-is-preferred-over-L2-norm?-1.1.14\">1.14 When L1 norm is preferred over L2 norm?</a></span></li><li><span><a href=\"#1.15-Can-the-number-of-nonzero-elements-in-a-vector-be-defined-as-L0-norm?\" data-toc-modified-id=\"1.15-Can-the-number-of-nonzero-elements-in-a-vector-be-defined-as-L0-norm?-1.1.15\">1.15 Can the number of nonzero elements in a vector be defined as L0 norm?</a></span></li><li><span><a href=\"#1.16-What-is-Frobenius-norm?\" data-toc-modified-id=\"1.16-What-is-Frobenius-norm?-1.1.16\">1.16 What is Frobenius norm?</a></span></li><li><span><a href=\"#1.17-What-is-a-diagonal-matrix?\" data-toc-modified-id=\"1.17-What-is-a-diagonal-matrix?-1.1.17\">1.17 What is a diagonal matrix?</a></span></li><li><span><a href=\"#1.18-Why-is-multiplication-by-diagonal-matrix-computationally-cheap?-How-is-the-multiplication-different-for-square-vs.-non-square-diagonal-matrix?\" data-toc-modified-id=\"1.18-Why-is-multiplication-by-diagonal-matrix-computationally-cheap?-How-is-the-multiplication-different-for-square-vs.-non-square-diagonal-matrix?-1.1.18\">1.18 Why is multiplication by diagonal matrix computationally cheap? How is the multiplication different for square vs. non-square diagonal matrix?</a></span></li><li><span><a href=\"#1.19-At-what-conditions-does-the-inverse-of-a-diagonal-matrix-exist?\" data-toc-modified-id=\"1.19-At-what-conditions-does-the-inverse-of-a-diagonal-matrix-exist?-1.1.19\">1.19 At what conditions does the inverse of a diagonal matrix exist?</a></span></li><li><span><a href=\"#1.20-What-is-a-symmetrix-matrix?\" data-toc-modified-id=\"1.20-What-is-a-symmetrix-matrix?-1.1.20\">1.20 What is a symmetrix matrix?</a></span></li><li><span><a href=\"#1.21-What-is-a-unit-vector?\" data-toc-modified-id=\"1.21-What-is-a-unit-vector?-1.1.21\">1.21 What is a unit vector?</a></span></li><li><span><a href=\"#1.22-When-are-two-vectors-x-and-y-orthogonal?\" data-toc-modified-id=\"1.22-When-are-two-vectors-x-and-y-orthogonal?-1.1.22\">1.22 When are two vectors x and y orthogonal?</a></span></li><li><span><a href=\"#1.23-What-is-the-maximum-possible-number-of-orthogonal-vectors-with-non-zero-norm?\" data-toc-modified-id=\"1.23-What-is-the-maximum-possible-number-of-orthogonal-vectors-with-non-zero-norm?-1.1.23\">1.23 What is the maximum possible number of orthogonal vectors with non-zero norm?</a></span></li><li><span><a href=\"#1.24-When-are-two-vectors-x-and-y-orthonormal?\" data-toc-modified-id=\"1.24-When-are-two-vectors-x-and-y-orthonormal?-1.1.24\">1.24 When are two vectors x and y orthonormal?</a></span></li><li><span><a href=\"#1.25-What-is-an-orthogonal-matrix?-Why-is-computationally-preferred?\" data-toc-modified-id=\"1.25-What-is-an-orthogonal-matrix?-Why-is-computationally-preferred?-1.1.25\">1.25 What is an orthogonal matrix? Why is computationally preferred?</a></span></li><li><span><a href=\"#1.26-What-is-eigendecomposition,-eigenvectors-and-eigenvalues?\" data-toc-modified-id=\"1.26-What-is-eigendecomposition,-eigenvectors-and-eigenvalues?-1.1.26\">1.26 What is eigendecomposition, eigenvectors and eigenvalues?</a></span><ul class=\"toc-item\"><li><span><a href=\"#1.26.1-Eigendecomposition-of-a-matrix\" data-toc-modified-id=\"1.26.1-Eigendecomposition-of-a-matrix-1.1.26.1\">1.26.1 Eigendecomposition of a matrix</a></span></li><li><span><a href=\"#1.26.2-Eigenvectors-and-Eigenvalues\" data-toc-modified-id=\"1.26.2-Eigenvectors-and-Eigenvalues-1.1.26.2\">1.26.2 Eigenvectors and Eigenvalues</a></span></li><li><span><a href=\"#1.26.3-code-examples\" data-toc-modified-id=\"1.26.3-code-examples-1.1.26.3\">1.26.3 code examples</a></span></li></ul></li><li><span><a href=\"#1.27-How-to-find-eigen-values-of-a-matrix?\" data-toc-modified-id=\"1.27-How-to-find-eigen-values-of-a-matrix?-1.1.27\">1.27 How to find eigen values of a matrix?</a></span></li><li><span><a href=\"#1.28-Write-the-eigendecomposition-formula-for-a-matrix.-If-the-matrix-is-real-symmetric,-how-will-this-change?\" data-toc-modified-id=\"1.28-Write-the-eigendecomposition-formula-for-a-matrix.-If-the-matrix-is-real-symmetric,-how-will-this-change?-1.1.28\">1.28 Write the eigendecomposition formula for a matrix. If the matrix is real symmetric, how will this change?</a></span></li><li><span><a href=\"#1.29-Is-the-Eigendecomposition-guaranteed-to-be-unique?-If-not,-then-how-do-we-represent-it?\" data-toc-modified-id=\"1.29-Is-the-Eigendecomposition-guaranteed-to-be-unique?-If-not,-then-how-do-we-represent-it?-1.1.29\">1.29 Is the Eigendecomposition guaranteed to be unique? If not, then how do we represent it?</a></span></li><li><span><a href=\"#1.30-What-are-positive-definite,-negative-definite,-positive-semi-definite-and-negative-semi-definite-matrices?\" data-toc-modified-id=\"1.30-What-are-positive-definite,-negative-definite,-positive-semi-definite-and-negative-semi-definite-matrices?-1.1.30\">1.30 What are positive definite, negative definite, positive semi definite and negative semi definite matrices?</a></span></li><li><span><a href=\"#1.31-What-is-Singular-Value-Decomposition?-Why-do-we-use-it?-Why-not-just-use-ED?\" data-toc-modified-id=\"1.31-What-is-Singular-Value-Decomposition?-Why-do-we-use-it?-Why-not-just-use-ED?-1.1.31\">1.31 What is Singular Value Decomposition? Why do we use it? Why not just use ED?</a></span></li><li><span><a href=\"#1.32-Given-a-matrix-A,-how-will-you-calculate-its-Singular-Value-Decomposition?\" data-toc-modified-id=\"1.32-Given-a-matrix-A,-how-will-you-calculate-its-Singular-Value-Decomposition?-1.1.32\">1.32 Given a matrix A, how will you calculate its Singular Value Decomposition?</a></span></li><li><span><a href=\"#1.33-What-are-singular-values,-left-singulars-and-right-singulars?\" data-toc-modified-id=\"1.33-What-are-singular-values,-left-singulars-and-right-singulars?-1.1.33\">1.33 What are singular values, left singulars and right singulars?</a></span></li><li><span><a href=\"#1.34-Why-are-singular-values-always-non-negative?\" data-toc-modified-id=\"1.34-Why-are-singular-values-always-non-negative?-1.1.34\">1.34 Why are singular values always non-negative?</a></span></li><li><span><a href=\"#1.36-What-is-the-Moore-Penrose-pseudo-inverse-and-how-to-calculate-it?\" data-toc-modified-id=\"1.36-What-is-the-Moore-Penrose-pseudo-inverse-and-how-to-calculate-it?-1.1.35\">1.36 What is the Moore Penrose pseudo inverse and how to calculate it?</a></span></li><li><span><a href=\"#1.37-If-we-do-Moore-Penrose-pseudo-inverse-on-Ax-=-b,-what-solution-is-provided-is-A-is-fat?-Moreover,-what-solution-is-provided-if-A-is-tall?\" data-toc-modified-id=\"1.37-If-we-do-Moore-Penrose-pseudo-inverse-on-Ax-=-b,-what-solution-is-provided-is-A-is-fat?-Moreover,-what-solution-is-provided-if-A-is-tall?-1.1.36\">1.37 If we do Moore Penrose pseudo inverse on Ax = b, what solution is provided is A is fat? Moreover, what solution is provided if A is tall?</a></span></li><li><span><a href=\"#1.38-Which-matrices-can-be-decomposed-by-ED?\" data-toc-modified-id=\"1.38-Which-matrices-can-be-decomposed-by-ED?-1.1.37\">1.38 Which matrices can be decomposed by ED?</a></span></li><li><span><a href=\"#1.39-Which-matrices-can-be-decomposed-by-SVD?\" data-toc-modified-id=\"1.39-Which-matrices-can-be-decomposed-by-SVD?-1.1.38\">1.39 Which matrices can be decomposed by SVD?</a></span></li><li><span><a href=\"#1.40-What-is-the-trace-of-a-matrix?\" data-toc-modified-id=\"1.40-What-is-the-trace-of-a-matrix?-1.1.39\">1.40 What is the trace of a matrix?</a></span></li><li><span><a href=\"#1.41-How-to-write-Frobenius-norm-of-a-matrix-A-in-terms-of-trace?\" data-toc-modified-id=\"1.41-How-to-write-Frobenius-norm-of-a-matrix-A-in-terms-of-trace?-1.1.40\">1.41 How to write Frobenius norm of a matrix A in terms of trace?</a></span></li><li><span><a href=\"#1.42-Why-is-trace-of-a-multiplication-of-matrices-invariant-to-cyclic-permutations?\" data-toc-modified-id=\"1.42-Why-is-trace-of-a-multiplication-of-matrices-invariant-to-cyclic-permutations?-1.1.41\">1.42 Why is trace of a multiplication of matrices invariant to cyclic permutations?</a></span></li><li><span><a href=\"#1.43-What-is-the-trace-of-a-scalar?\" data-toc-modified-id=\"1.43-What-is-the-trace-of-a-scalar?-1.1.42\">1.43 What is the trace of a scalar?</a></span></li><li><span><a href=\"#1.44-Write-the-frobenius-norm-of-a-matrix-in-terms-of-trace?\" data-toc-modified-id=\"1.44-Write-the-frobenius-norm-of-a-matrix-in-terms-of-trace?-1.1.43\">1.44 Write the frobenius norm of a matrix in terms of trace?</a></span></li></ul></li><li><span><a href=\"#Part-2-:-Numerical-Optimization\" data-toc-modified-id=\"Part-2-:-Numerical-Optimization-1.2\">Part 2 : Numerical Optimization</a></span><ul class=\"toc-item\"><li><span><a href=\"#2.1-What-is-underflow-and-overflow?\" data-toc-modified-id=\"2.1-What-is-underflow-and-overflow?-1.2.1\">2.1 What is underflow and overflow?</a></span></li><li><span><a href=\"#2.2-How-to-tackle-the-problem-of-underflow-or-overflow-for-softmax-function-or-log-softmax-function?\" data-toc-modified-id=\"2.2-How-to-tackle-the-problem-of-underflow-or-overflow-for-softmax-function-or-log-softmax-function?-1.2.2\">2.2 How to tackle the problem of underflow or overflow for softmax function or log softmax function?</a></span></li><li><span><a href=\"#2.3-What-is-poor-conditioning?\" data-toc-modified-id=\"2.3-What-is-poor-conditioning?-1.2.3\">2.3 What is poor conditioning?</a></span></li><li><span><a href=\"#2.4-What-is-the-condition-number?\" data-toc-modified-id=\"2.4-What-is-the-condition-number?-1.2.4\">2.4 What is the condition number?</a></span></li><li><span><a href=\"#2.5-What-are-grad,-div-and-curl?\" data-toc-modified-id=\"2.5-What-are-grad,-div-and-curl?-1.2.5\">2.5 What are grad, div and curl?</a></span></li><li><span><a href=\"#2.6-What-are-critical-or-stationary-points-in-multi-dimensions?\" data-toc-modified-id=\"2.6-What-are-critical-or-stationary-points-in-multi-dimensions?-1.2.6\">2.6 What are critical or stationary points in multi-dimensions?</a></span></li><li><span><a href=\"#2.7-Why-should-you-do-gradient-descent-when-you-want-to-minimize-a-function?\" data-toc-modified-id=\"2.7-Why-should-you-do-gradient-descent-when-you-want-to-minimize-a-function?-1.2.7\">2.7 Why should you do gradient descent when you want to minimize a function?</a></span></li><li><span><a href=\"#2.8-What-is-line-search?\" data-toc-modified-id=\"2.8-What-is-line-search?-1.2.8\">2.8 What is line search?</a></span></li><li><span><a href=\"#2.9-What-is-hill-climbing?\" data-toc-modified-id=\"2.9-What-is-hill-climbing?-1.2.9\">2.9 What is hill climbing?</a></span></li><li><span><a href=\"#2.10-What-is-a-Jacobian-matrix?\" data-toc-modified-id=\"2.10-What-is-a-Jacobian-matrix?-1.2.10\">2.10 What is a Jacobian matrix?</a></span></li><li><span><a href=\"#2.11-What-is-curvature?\" data-toc-modified-id=\"2.11-What-is-curvature?-1.2.11\">2.11 What is curvature?</a></span></li><li><span><a href=\"#2.12-What-is-a-Hessian-matrix?\" data-toc-modified-id=\"2.12-What-is-a-Hessian-matrix?-1.2.12\">2.12 What is a Hessian matrix?</a></span></li></ul></li><li><span><a href=\"#Part-3-:-Basics-of-Probability-and-Information-Theory\" data-toc-modified-id=\"Part-3-:-Basics-of-Probability-and-Information-Theory-1.3\">Part 3 : Basics of Probability and Information Theory</a></span><ul class=\"toc-item\"><li><span><a href=\"#3.1-Compare-“Frequentist-probability”-vs.-“Bayesian-probability”?\" data-toc-modified-id=\"3.1-Compare-“Frequentist-probability”-vs.-“Bayesian-probability”?-1.3.1\">3.1 Compare “Frequentist probability” vs. “Bayesian probability”?</a></span></li><li><span><a href=\"#3.2-What-is-a-random-variable?\" data-toc-modified-id=\"3.2-What-is-a-random-variable?-1.3.2\">3.2 What is a random variable?</a></span></li><li><span><a href=\"#3.3-What-is-a-probability-distribution?\" data-toc-modified-id=\"3.3-What-is-a-probability-distribution?-1.3.3\">3.3 What is a probability distribution?</a></span></li><li><span><a href=\"#3.4-What-is-a-probability-mass-function?\" data-toc-modified-id=\"3.4-What-is-a-probability-mass-function?-1.3.4\">3.4 What is a probability mass function?</a></span></li><li><span><a href=\"#3.5-What-is-a-probability-density-function?\" data-toc-modified-id=\"3.5-What-is-a-probability-density-function?-1.3.5\">3.5 What is a probability density function?</a></span></li><li><span><a href=\"#3.6-What-is-a-joint-probability-distribution?\" data-toc-modified-id=\"3.6-What-is-a-joint-probability-distribution?-1.3.6\">3.6 What is a joint probability distribution?</a></span></li><li><span><a href=\"#3.7-What-are-the-conditions-for-a-function-to-be-a-probability-mass-function?\" data-toc-modified-id=\"3.7-What-are-the-conditions-for-a-function-to-be-a-probability-mass-function?-1.3.7\">3.7 What are the conditions for a function to be a probability mass function?</a></span></li><li><span><a href=\"#3.8-What-are-the-conditions-for-a-function-to-be-a-probability-density-function?\" data-toc-modified-id=\"3.8-What-are-the-conditions-for-a-function-to-be-a-probability-density-function?-1.3.8\">3.8 What are the conditions for a function to be a probability density function?</a></span></li><li><span><a href=\"#3.9-What-is-a-marginal-probability?-Given-the-joint-probability-function,-how-will-you-calculate-it?\" data-toc-modified-id=\"3.9-What-is-a-marginal-probability?-Given-the-joint-probability-function,-how-will-you-calculate-it?-1.3.9\">3.9 What is a marginal probability? Given the joint probability function, how will you calculate it?</a></span></li><li><span><a href=\"#3.10-What-is-conditional-probability?-Given-the-joint-probability-function,-how-will-you-calculate-it?\" data-toc-modified-id=\"3.10-What-is-conditional-probability?-Given-the-joint-probability-function,-how-will-you-calculate-it?-1.3.10\">3.10 What is conditional probability? Given the joint probability function, how will you calculate it?</a></span></li><li><span><a href=\"#3.11-State-the-Chain-rule-of-conditional-probabilities.\" data-toc-modified-id=\"3.11-State-the-Chain-rule-of-conditional-probabilities.-1.3.11\">3.11 State the Chain rule of conditional probabilities.</a></span></li><li><span><a href=\"#3.12-What-are-the-conditions-for-independence-and-conditional-independence-of-two-random-variables?\" data-toc-modified-id=\"3.12-What-are-the-conditions-for-independence-and-conditional-independence-of-two-random-variables?-1.3.12\">3.12 What are the conditions for independence and conditional independence of two random variables?</a></span></li><li><span><a href=\"#3.13-What-are-expectation,-variance-and-covariance?\" data-toc-modified-id=\"3.13-What-are-expectation,-variance-and-covariance?-1.3.13\">3.13 What are expectation, variance and covariance?</a></span></li><li><span><a href=\"#3.14-Compare-covariance-and-independence.\" data-toc-modified-id=\"3.14-Compare-covariance-and-independence.-1.3.14\">3.14 Compare covariance and independence.</a></span></li><li><span><a href=\"#3.15-What-is-the-covariance-for-a-vector-of-random-variables?\" data-toc-modified-id=\"3.15-What-is-the-covariance-for-a-vector-of-random-variables?-1.3.15\">3.15 What is the covariance for a vector of random variables?</a></span></li><li><span><a href=\"#3.16-What-is-a-Bernoulli-distribution?-Calculate-the-expectation-and-variance-of-a-random-variable-that-follows-Bernoulli-distribution?\" data-toc-modified-id=\"3.16-What-is-a-Bernoulli-distribution?-Calculate-the-expectation-and-variance-of-a-random-variable-that-follows-Bernoulli-distribution?-1.3.16\">3.16 What is a Bernoulli distribution? Calculate the expectation and variance of a random variable that follows Bernoulli distribution?</a></span></li><li><span><a href=\"#3.17-What-is-a-multinoulli-distribution?\" data-toc-modified-id=\"3.17-What-is-a-multinoulli-distribution?-1.3.17\">3.17 What is a multinoulli distribution?</a></span></li><li><span><a href=\"#3.18-What-is-a-normal-distribution?\" data-toc-modified-id=\"3.18-What-is-a-normal-distribution?-1.3.18\">3.18 What is a normal distribution?</a></span></li><li><span><a href=\"#3.19-Why-is-the-normal-distribution-a-default-choice-for-a-prior-over-a-set-of-real-numbers?\" data-toc-modified-id=\"3.19-Why-is-the-normal-distribution-a-default-choice-for-a-prior-over-a-set-of-real-numbers?-1.3.19\">3.19 Why is the normal distribution a default choice for a prior over a set of real numbers?</a></span></li><li><span><a href=\"#3.20-What-is-the-central-limit-theorem?\" data-toc-modified-id=\"3.20-What-is-the-central-limit-theorem?-1.3.20\">3.20 What is the central limit theorem?</a></span></li><li><span><a href=\"#3.21-What-are-exponential-and-Laplace-distribution?\" data-toc-modified-id=\"3.21-What-are-exponential-and-Laplace-distribution?-1.3.21\">3.21 What are exponential and Laplace distribution?</a></span></li><li><span><a href=\"#3.22-What-are-Dirac-distribution-and-Empirical-distribution?\" data-toc-modified-id=\"3.22-What-are-Dirac-distribution-and-Empirical-distribution?-1.3.22\">3.22 What are Dirac distribution and Empirical distribution?</a></span></li><li><span><a href=\"#3.23-What-is-mixture-of-distributions?\" data-toc-modified-id=\"3.23-What-is-mixture-of-distributions?-1.3.23\">3.23 What is mixture of distributions?</a></span></li><li><span><a href=\"#3.24-Name-two-common-examples-of-mixture-of-distributions?-(Empirical-and-Gaussian-Mixture)\" data-toc-modified-id=\"3.24-Name-two-common-examples-of-mixture-of-distributions?-(Empirical-and-Gaussian-Mixture)-1.3.24\">3.24 Name two common examples of mixture of distributions? (Empirical and Gaussian Mixture)</a></span></li><li><span><a href=\"#3.25-Is-Gaussian-mixture-model-a-universal-approximator-of-densities?\" data-toc-modified-id=\"3.25-Is-Gaussian-mixture-model-a-universal-approximator-of-densities?-1.3.25\">3.25 Is Gaussian mixture model a universal approximator of densities?</a></span></li><li><span><a href=\"#3.26-Write-the-formulae-for-logistic-and-softplus-function.\" data-toc-modified-id=\"3.26-Write-the-formulae-for-logistic-and-softplus-function.-1.3.26\">3.26 Write the formulae for logistic and softplus function.</a></span></li><li><span><a href=\"#3.27-Write-the-formulae-for-Bayes-rule.\" data-toc-modified-id=\"3.27-Write-the-formulae-for-Bayes-rule.-1.3.27\">3.27 Write the formulae for Bayes rule.</a></span></li><li><span><a href=\"#3.28-What-do-you-mean-by-measure-zero-and-almost-everywhere?\" data-toc-modified-id=\"3.28-What-do-you-mean-by-measure-zero-and-almost-everywhere?-1.3.28\">3.28 What do you mean by measure zero and almost everywhere?</a></span></li><li><span><a href=\"#3.29-If-two-random-variables-are-related-in-a-deterministic-way,-how-are-the-PDFs-related?\" data-toc-modified-id=\"3.29-If-two-random-variables-are-related-in-a-deterministic-way,-how-are-the-PDFs-related?-1.3.29\">3.29 If two random variables are related in a deterministic way, how are the PDFs related?</a></span></li><li><span><a href=\"#3.30-Define-self-information.-What-are-its-units?\" data-toc-modified-id=\"3.30-Define-self-information.-What-are-its-units?-1.3.30\">3.30 Define self-information. What are its units?</a></span></li><li><span><a href=\"#3.31-What-are-Shannon-entropy-and-differential-entropy?\" data-toc-modified-id=\"3.31-What-are-Shannon-entropy-and-differential-entropy?-1.3.31\">3.31 What are Shannon entropy and differential entropy?</a></span></li><li><span><a href=\"#3.32-What-is-Kullback-Leibler-(KL)-divergence?\" data-toc-modified-id=\"3.32-What-is-Kullback-Leibler-(KL)-divergence?-1.3.32\">3.32 What is Kullback-Leibler (KL) divergence?</a></span></li><li><span><a href=\"#3.33-Can-KL-divergence-be-used-as-a-distance-measure?\" data-toc-modified-id=\"3.33-Can-KL-divergence-be-used-as-a-distance-measure?-1.3.33\">3.33 Can KL divergence be used as a distance measure?</a></span></li><li><span><a href=\"#3.34-Define-cross-entropy.\" data-toc-modified-id=\"3.34-Define-cross-entropy.-1.3.34\">3.34 Define cross-entropy.</a></span></li><li><span><a href=\"#3.35-What-are-structured-probabilistic-models-or-graphical-models?\" data-toc-modified-id=\"3.35-What-are-structured-probabilistic-models-or-graphical-models?-1.3.35\">3.35 What are structured probabilistic models or graphical models?</a></span></li><li><span><a href=\"#3.36-In-the-context-of-structured-probabilistic-models,-what-are-directed-and-undirected-models?-How-are-they-represented?-What-are-cliques-in-undirected-structured-probabilistic-models?\" data-toc-modified-id=\"3.36-In-the-context-of-structured-probabilistic-models,-what-are-directed-and-undirected-models?-How-are-they-represented?-What-are-cliques-in-undirected-structured-probabilistic-models?-1.3.36\">3.36 In the context of structured probabilistic models, what are directed and undirected models? How are they represented? What are cliques in undirected structured probabilistic models?</a></span></li></ul></li><li><span><a href=\"#Part-4-:-Confidence-interval\" data-toc-modified-id=\"Part-4-:-Confidence-interval-1.4\">Part 4 : Confidence interval</a></span><ul class=\"toc-item\"><li><span><a href=\"#4.1-What-is-population-mean-and-sample-mean?\" data-toc-modified-id=\"4.1-What-is-population-mean-and-sample-mean?-1.4.1\">4.1 What is population mean and sample mean?</a></span></li><li><span><a href=\"#4.2-What-is-population-standard-deviation-and-sample-standard-deviation?\" data-toc-modified-id=\"4.2-What-is-population-standard-deviation-and-sample-standard-deviation?-1.4.2\">4.2 What is population standard deviation and sample standard deviation?</a></span></li><li><span><a href=\"#4.3-Why-population-s.d.-has-N-degrees-of-freedom-while-sample-s.d.-has-N-1-degrees-of-freedom?-In-other-words,-why-1/N-inside-root-for-pop.-s.d.-and-1/(N-1)-inside-root-for-sample-s.d.?\" data-toc-modified-id=\"4.3-Why-population-s.d.-has-N-degrees-of-freedom-while-sample-s.d.-has-N-1-degrees-of-freedom?-In-other-words,-why-1/N-inside-root-for-pop.-s.d.-and-1/(N-1)-inside-root-for-sample-s.d.?-1.4.3\">4.3 Why population s.d. has N degrees of freedom while sample s.d. has N-1 degrees of freedom? In other words, why 1/N inside root for pop. s.d. and 1/(N-1) inside root for sample s.d.?</a></span></li><li><span><a href=\"#4.4-What-is-the-formula-for-calculating-the-s.d.-of-the-sample-mean?\" data-toc-modified-id=\"4.4-What-is-the-formula-for-calculating-the-s.d.-of-the-sample-mean?-1.4.4\">4.4 What is the formula for calculating the s.d. of the sample mean?</a></span></li><li><span><a href=\"#4.5-What-is-confidence-interval?\" data-toc-modified-id=\"4.5-What-is-confidence-interval?-1.4.5\">4.5 What is confidence interval?</a></span></li><li><span><a href=\"#4.6-What-is-standard-error?\" data-toc-modified-id=\"4.6-What-is-standard-error?-1.4.6\">4.6 What is standard error?</a></span></li></ul></li><li><span><a href=\"#Part-5-:-Learning-Theory\" data-toc-modified-id=\"Part-5-:-Learning-Theory-1.5\">Part 5 : Learning Theory</a></span><ul class=\"toc-item\"><li><span><a href=\"#5.1-Describe-bias-and-variance-with-examples.\" data-toc-modified-id=\"5.1-Describe-bias-and-variance-with-examples.-1.5.1\">5.1 Describe bias and variance with examples.</a></span></li><li><span><a href=\"#5.2-What-is-Empirical-Risk-Minimization?\" data-toc-modified-id=\"5.2-What-is-Empirical-Risk-Minimization?-1.5.2\">5.2 What is Empirical Risk Minimization?</a></span></li><li><span><a href=\"#5.3-What-is-Union-bound-and-Hoeffding’s-inequality?\" data-toc-modified-id=\"5.3-What-is-Union-bound-and-Hoeffding’s-inequality?-1.5.3\">5.3 What is Union bound and Hoeffding’s inequality?</a></span></li><li><span><a href=\"#5.4-Write-the-formulae-for-training-error-and-generalization-error.-Point-out-the-differences.\" data-toc-modified-id=\"5.4-Write-the-formulae-for-training-error-and-generalization-error.-Point-out-the-differences.-1.5.4\">5.4 Write the formulae for training error and generalization error. Point out the differences.</a></span></li><li><span><a href=\"#5.5-State-the-uniform-convergence-theorem-and-derive-it.\" data-toc-modified-id=\"5.5-State-the-uniform-convergence-theorem-and-derive-it.-1.5.5\">5.5 State the uniform convergence theorem and derive it.</a></span></li><li><span><a href=\"#5.6-What-is-sample-complexity-bound-of-uniform-convergence-theorem?\" data-toc-modified-id=\"5.6-What-is-sample-complexity-bound-of-uniform-convergence-theorem?-1.5.6\">5.6 What is sample complexity bound of uniform convergence theorem?</a></span></li><li><span><a href=\"#5.7-What-is-error-bound-of-uniform-convergence-theorem?\" data-toc-modified-id=\"5.7-What-is-error-bound-of-uniform-convergence-theorem?-1.5.7\">5.7 What is error bound of uniform convergence theorem?</a></span></li><li><span><a href=\"#5.8-What-is-the-bias-variance-trade-off-theorem?\" data-toc-modified-id=\"5.8-What-is-the-bias-variance-trade-off-theorem?-1.5.8\">5.8 What is the bias-variance trade-off theorem?</a></span></li><li><span><a href=\"#5.9-From-the-bias-variance-trade-off,-can-you-derive-the-bound-on-training-set-size?\" data-toc-modified-id=\"5.9-From-the-bias-variance-trade-off,-can-you-derive-the-bound-on-training-set-size?-1.5.9\">5.9 From the bias-variance trade-off, can you derive the bound on training set size?</a></span></li><li><span><a href=\"#5.10-What-is-the-VC-dimension?\" data-toc-modified-id=\"5.10-What-is-the-VC-dimension?-1.5.10\">5.10 What is the VC dimension?</a></span></li><li><span><a href=\"#5.11-What-does-the-training-set-size-depend-on-for-a-finite-and-infinite-hypothesis-set?-Compare-and-contrast.\" data-toc-modified-id=\"5.11-What-does-the-training-set-size-depend-on-for-a-finite-and-infinite-hypothesis-set?-Compare-and-contrast.-1.5.11\">5.11 What does the training set size depend on for a finite and infinite hypothesis set? Compare and contrast.</a></span></li><li><span><a href=\"#5.12-What-is-the-VC-dimension-for-an-n-dimensional-linear-classifier?\" data-toc-modified-id=\"5.12-What-is-the-VC-dimension-for-an-n-dimensional-linear-classifier?-1.5.12\">5.12 What is the VC dimension for an n-dimensional linear classifier?</a></span></li><li><span><a href=\"#5.13-How-is-the-VC-dimension-of-a-SVM-bounded-although-it-is-projected-to-an-infinite-dimension?\" data-toc-modified-id=\"5.13-How-is-the-VC-dimension-of-a-SVM-bounded-although-it-is-projected-to-an-infinite-dimension?-1.5.13\">5.13 How is the VC dimension of a SVM bounded although it is projected to an infinite dimension?</a></span></li><li><span><a href=\"#5.14-Considering-that-Empirical-Risk-Minimization-is-a-NP-hard-problem,-how-does-logistic-regression-and-SVM-loss-work?\" data-toc-modified-id=\"5.14-Considering-that-Empirical-Risk-Minimization-is-a-NP-hard-problem,-how-does-logistic-regression-and-SVM-loss-work?-1.5.14\">5.14 Considering that Empirical Risk Minimization is a NP-hard problem, how does logistic regression and SVM loss work?</a></span></li></ul></li><li><span><a href=\"#Part-6-:-Model-and-feature-selection\" data-toc-modified-id=\"Part-6-:-Model-and-feature-selection-1.6\">Part 6 : Model and feature selection</a></span><ul class=\"toc-item\"><li><span><a href=\"#6.1-Why-are-model-selection-methods-needed?\" data-toc-modified-id=\"6.1-Why-are-model-selection-methods-needed?-1.6.1\">6.1 Why are model selection methods needed?</a></span></li><li><span><a href=\"#6.2-How-do-you-do-a-trade-off-between-bias-and-variance?\" data-toc-modified-id=\"6.2-How-do-you-do-a-trade-off-between-bias-and-variance?-1.6.2\">6.2 How do you do a trade-off between bias and variance?</a></span></li><li><span><a href=\"#6.3-What-are-the-different-attributes-that-can-be-selected-by-model-selection-methods?\" data-toc-modified-id=\"6.3-What-are-the-different-attributes-that-can-be-selected-by-model-selection-methods?-1.6.3\">6.3 What are the different attributes that can be selected by model selection methods?</a></span></li><li><span><a href=\"#6.4-Why-is-cross-validation-required?\" data-toc-modified-id=\"6.4-Why-is-cross-validation-required?-1.6.4\">6.4 Why is cross-validation required?</a></span></li><li><span><a href=\"#6.5-Describe-different-cross-validation-techniques.\" data-toc-modified-id=\"6.5-Describe-different-cross-validation-techniques.-1.6.5\">6.5 Describe different cross-validation techniques.</a></span></li><li><span><a href=\"#6.6-What-is-hold-out-cross-validation?-What-are-its-advantages-and-disadvantages?\" data-toc-modified-id=\"6.6-What-is-hold-out-cross-validation?-What-are-its-advantages-and-disadvantages?-1.6.6\">6.6 What is hold-out cross validation? What are its advantages and disadvantages?</a></span></li><li><span><a href=\"#6.7-What-is-k-fold-cross-validation?-What-are-its-advantages-and-disadvantages?\" data-toc-modified-id=\"6.7-What-is-k-fold-cross-validation?-What-are-its-advantages-and-disadvantages?-1.6.7\">6.7 What is k-fold cross validation? What are its advantages and disadvantages?</a></span></li><li><span><a href=\"#6.8-What-is-leave-one-out-cross-validation?-What-are-its-advantages-and-disadvantages?\" data-toc-modified-id=\"6.8-What-is-leave-one-out-cross-validation?-What-are-its-advantages-and-disadvantages?-1.6.8\">6.8 What is leave-one-out cross validation? What are its advantages and disadvantages?</a></span></li><li><span><a href=\"#6.9-Why-is-feature-selection-required?\" data-toc-modified-id=\"6.9-Why-is-feature-selection-required?-1.6.9\">6.9 Why is feature selection required?</a></span></li><li><span><a href=\"#6.10-Describe-some-feature-selection-methods.\" data-toc-modified-id=\"6.10-Describe-some-feature-selection-methods.-1.6.10\">6.10 Describe some feature selection methods.</a></span></li><li><span><a href=\"#6.11-What-is-forward-feature-selection-method?-What-are-its-advantages-and-disadvantages?\" data-toc-modified-id=\"6.11-What-is-forward-feature-selection-method?-What-are-its-advantages-and-disadvantages?-1.6.11\">6.11 What is forward feature selection method? What are its advantages and disadvantages?</a></span></li><li><span><a href=\"#6.12-What-is-backward-feature-selection-method?-What-are-its-advantages-and-disadvantages?\" data-toc-modified-id=\"6.12-What-is-backward-feature-selection-method?-What-are-its-advantages-and-disadvantages?-1.6.12\">6.12 What is backward feature selection method? What are its advantages and disadvantages?</a></span></li><li><span><a href=\"#6.13-What-is-filter-feature-selection-method-and-describe-two-of-them?\" data-toc-modified-id=\"6.13-What-is-filter-feature-selection-method-and-describe-two-of-them?-1.6.13\">6.13 What is filter feature selection method and describe two of them?</a></span></li><li><span><a href=\"#6.14-What-is-mutual-information-and-KL-divergence?\" data-toc-modified-id=\"6.14-What-is-mutual-information-and-KL-divergence?-1.6.14\">6.14 What is mutual information and KL divergence?</a></span></li><li><span><a href=\"#6.15-Describe-KL-divergence-intuitively.\" data-toc-modified-id=\"6.15-Describe-KL-divergence-intuitively.-1.6.15\">6.15 Describe KL divergence intuitively.</a></span></li></ul></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Questions / Answers in Machine Learning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 : Linear Algebra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 What is broadcasting in connection to Linear Algebra?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Broadcasting is the process of making arrays to have compatible shapes for arithmetic operations. Two shapes are compatible if for each dimension pair they are either equal or one of them is one. When trying to broadcast a Tensor to a shape, it starts with the trailing dimensions, and works its way forward.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 What are scalars, vectors, matrices, and tensors?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 What is Hadamard product of two matrices?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Hadamard product (also known as the element-wise, entrywise or Schur product) is a binary operation that takes two matrices of the same dimensions and produces another matrix of the same dimension as the operands where each element i, j is the product of elements i, j of the original two matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Image of Hadamard Product](https://miro.medium.com/max/1400/1*54rq3_-FZaJxKLdOYN8qjA.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 What is an inverse matrix ? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we multiply a number by its reciprocal/inverse we get 1. When we multiply a matrix by its inverse we get the Identity Matrix (which is like \"1\" for matrices)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5  If inverse of a matrix exists, how to calculate it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[method to compute inverse of a matrix](https://www.mathsisfun.com/algebra/matrix-inverse.html)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6 What is the determinant of a square matrix? How is it calculated? What is the connection of determinant to eigenvalues?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[method to compute determinant of a square matrix](https://www.mathsisfun.com/algebra/matrix-determinant.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determinants aren't really that important practically because they're insanely difficult to calculate on large matrices. However, they do tell you whether a matrix is invertible or not.\n",
    "\n",
    "Eigenvalues have several purposes, one of which is that the product of the eigenvalues is the determinant. Since eigenvalues are far easier to compute, they're pretty significant, especially to engineers.\n",
    "\n",
    "In addition to that, since any system of differential equations can be approximated by a linear system via the jacobian, the eigenvalues and eigenvectors can reveal how a system is behaving locally. So as an engineer you will be working with eigenvalues for the rest of your life."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.7 Discuss span and linear dependence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.8 What is Ax = b? When does Ax =b has a unique solution?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Orthonormal Vectors, Orthogonal Matrices and Hadamard Matrix](https://medium.com/linear-algebra/part-23-orthonormal-vectors-orthogonal-matrices-and-hadamard-matrix-bee6857c05c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.9 In Ax = b, what happens when A is fat or tall? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.10 When does inverse of A exist?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inverse of a matrix exists when the matrix is invertible. Now for a matrix to be invertible , you need to have the condition that the determinant of the matrix must not be zero. That is $det(A) ≠ 0$ where $A$ is your matrix of interest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.11 What is a norm? What is L1, L2 and L infinity norm?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Linear Algebra, a Norm refers to the total length of all the vectors in a space.\n",
    "\n",
    "\n",
    "L0 : Not a Norm. Corresponds to the total number of nonzero elements in a vector.\n",
    "For example, the L0 norm of the vectors (0,0) and (0,2) is 1 because there is only one nonzero element.\n",
    "\n",
    "L1 : Also known as Manhattan Distance or Taxicab norm. L1 Norm is the sum of the magnitudes of the vectors in a space. It is the most natural way of measure distance between vectors, that is the sum of absolute difference of the components of the vectors. In this norm, all the components of the vector are weighted equally.What is a Norm? In Linear Algebra, a Norm refers to the total length of all the vectors in a space.\n",
    "\n",
    "\n",
    "L2 : Is the most popular norm, also known as the Euclidean norm. It is the shortest distance to go from one point to another.\n",
    "\n",
    "L-infinity : Gives the largest magnitude among each element of a vector.Having the vector X= [-6, 4, 2], the L-infinity norm is 6. In L-infinity norm, only the largest element has any effect. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.12 What are the conditions a norm has to satisfy?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Norm is a function that returns length/size of any vector (except zero vector). If norm of x is greater than 0 then x is not equal to 0 (Zero Vector) and if norm is equal to 0 then x is a zero vector. If these three conditions are satisfied then the function f is a norm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.13 Why is squared of L2 norm preferred in ML than just L2 norm?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "squared L2 norm is computationally more simple, as you dont have to calculate the square root"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.14 When L1 norm is preferred over L2 norm?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L1 norm for feature pruning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.15 Can the number of nonzero elements in a vector be defined as L0 norm?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The L0 norm of a finite dimensional vector is the number of non-zero entries in it. Its not a norm in the rigorous sense, but intuitively it gives you a useful tool for comparing two vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.16 What is Frobenius norm?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take this matrix: <br/>\n",
    "[[2, −1] <br/>\n",
    "[-1, 2]]<br/>\n",
    " \n",
    "Its Frobenius norm is sqrt(10), but its eigenvalues are 3,1 so its 2-norm (or spectral radius) is 3. The Frobenius norm is always at least as large as the spectral radius. The Frobenius norm is at most r√ as much as the spectral radius, and this is probably tight (see the section on equivalence of norms in Wikipedia).\n",
    "\n",
    "Note that the Schatten 2-norm is equal to the Frobenius norm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-18T00:06:08.166145Z",
     "start_time": "2020-04-18T00:06:08.118273Z"
    }
   },
   "source": [
    "![Image of different norms](https://i.stack.imgur.com/voFvR.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.17 What is a diagonal matrix?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In linear algebra, a diagonal matrix is a matrix in which the entries outside the main diagonal are all zero; the term usually refers to square matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.18 Why is multiplication by diagonal matrix computationally cheap? How is the multiplication different for square vs. non-square diagonal matrix?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[computationally efficient matrices and matrix decompositions](https://medium.com/swlh/computationally-efficient-matrices-and-matrix-decompositions-64a134d76a9a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.19 At what conditions does the inverse of a diagonal matrix exist?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[how to find inverse of matrix](https://stattrek.com/matrix-algebra/find-matrix-inverse.aspx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the determinant of the matrix A (detA) is not zero, then this matrix has an inverse matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.20 What is a symmetrix matrix?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In linear algebra, a symmetric matrix is a square matrix that is equal to its transpose. Formally, Because equal matrices have equal dimensions, only square matrices can be symmetric. The entries of a symmetric matrix are symmetric with respect to the main diagonal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.21 What is a unit vector?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every nonzero vector has a corresponding unit vector, which has the same direction as that vector but a magnitude of 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[finding the unit vector](https://www.dummies.com/education/math/calculus/finding-the-unit-vector-of-a-vector/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.22 When are two vectors x and y orthogonal?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When dot product of x and y equals 0. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.23 What is the maximum possible number of orthogonal vectors with non-zero norm?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.24 When are two vectors x and y orthonormal?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If vector x and vector y are also unit vectors then they are orthonormal.\n",
    "To summarize, for a set of vectors to be orthogonal :\n",
    "They should be mutually perpendicular to each other (subtended at an angle of 90 degrees with each other).<br/>\n",
    "\n",
    "For a set of vectors to be orthonormal :<br/>\n",
    "1 : They should be unit vectors.<br/>\n",
    "2 : They should be orthogonal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.25 What is an orthogonal matrix? Why is computationally preferred?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An Orthogonal matrix is a type of square matrix whose columns and rows are orthonormal unit vectors, e.g. perpendicular and have a length or magnitude of 1. Because it's computationally cheap to calculate inverse matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.26 What is eigendecomposition, eigenvectors and eigenvalues?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.26.1 Eigendecomposition of a matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Eigendecomposition of a Matrix**<br/>\n",
    "\n",
    "Eigendecomposition of a matrix is a type of decomposition that involves decomposing a square matrix into a set of eigenvectors and eigenvalues.<br/>\n",
    "\n",
    "\"One of the most widely used kinds of matrix decomposition is called eigendecomposition, in which we decompose a matrix into a set of eigenvectors and eigenvalues.\" — Page 42, Deep Learning, 2016.<br/>\n",
    "\n",
    "A vector is an eigenvector of a matrix if it satisfies the following equation.<br/>\n",
    "\n",
    "A . v = lambda . v<br/>\n",
    "\n",
    "This is called the eigenvalue equation, where A is the parent square matrix that we are decomposing, v is the eigenvector of the matrix, and lambda is the lowercase Greek letter and represents the eigenvalue scalar.<br/>\n",
    "\n",
    "Or without the dot notation:<br/>\n",
    "\n",
    "Av = lambdav<br/>\n",
    "\n",
    "A matrix could have one eigenvector and eigenvalue for each dimension of the parent matrix. Not all square matrices can be decomposed into eigenvectors and eigenvalues, and some can only be decomposed in a way that requires complex numbers. The parent matrix can be shown to be a product of the eigenvectors and eigenvalues.<br/>\n",
    "\n",
    "$A = Q . diag(V) . Q^{-1}$<br/>\n",
    "\n",
    "Or, without the dot notation.<br/>\n",
    "\n",
    "$A = Qdiag(V)Q^{-1}$<br/>\n",
    "\n",
    "Where Q is a matrix comprised of the eigenvectors, diag(V) is a diagonal matrix comprised of the eigenvalues along the diagonal (sometimes represented with a capital lambda), and $Q^{-1}$ is the inverse of the matrix comprised of the eigenvectors.<br/>\n",
    "\n",
    "However, we often want to decompose matrices into their eigenvalues and eigenvectors. Doing so can help us to analyze certain properties of the matrix, much as decomposing an integer into its prime factors can help us understand the behavior of that integer. — Page 43, Deep Learning, 2016.<br/>\n",
    "\n",
    "Eigen is not a name, e.g. the method is not named after “Eigen”; eigen (pronounced eye-gan) is a German word that means “own” or “innate”, as in belonging to the parent matrix.<br/>\n",
    "\n",
    "A decomposition operation does not result in a compression of the matrix; instead, it breaks it down into constituent parts to make certain operations on the matrix easier to perform. Like other matrix decomposition methods, Eigendecomposition is used as an element to simplify the calculation of other more complex matrix operations.<br/>\n",
    "\n",
    "\"Almost all vectors change direction, when they are multiplied by A. Certain exceptional vectors x are in the same direction as Ax. Those are the “eigenvectors”. Multiply an eigenvector by A, and the vector Ax is the number lambda times the original x. The eigenvalue lambda tells whether the special vector x is stretched or shrunk or reversed or left unchanged – when it is multiplied by A.\" — Page 289, Introduction to Linear Algebra, Fifth Edition, 2016.<br/>\n",
    "\n",
    "Eigendecomposition can also be used to calculate the principal components of a matrix in the Principal Component Analysis method or PCA that can be used to reduce the dimensionality of data in machine learning.<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.26.2 Eigenvectors and Eigenvalues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eigenvectors are unit vectors, which means that their length or magnitude is equal to 1.0. They are often referred as right vectors, which simply means a column vector (as opposed to a row vector or a left vector). A right-vector is a vector as we understand them.<br/>\n",
    "\n",
    "Eigenvalues are coefficients applied to eigenvectors that give the vectors their length or magnitude. For example, a negative eigenvalue may reverse the direction of the eigenvector as part of scaling it.<br/>\n",
    "\n",
    "A matrix that has only positive eigenvalues is referred to as a positive definite matrix, whereas if the eigenvalues are all negative, it is referred to as a negative definite matrix.<br/>\n",
    "\n",
    "\"Decomposing a matrix in terms of its eigenvalues and its eigenvectors gives valuable insights into the properties of the matrix. Certain matrix calculations, like computing the power of the matrix, become much easier when we use the eigendecomposition of the matrix.\" — Page 262, No Bullshit Guide To Linear Algebra, 2017"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.26.3 code examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.26.3.1 Calculation of Eigendecomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An eigendecomposition is calculated on a square matrix using an efficient iterative algorithm, of which we will not go into the details.\n",
    "\n",
    "Often an eigenvalue is found first, then an eigenvector is found to solve the equation as a set of coefficients.\n",
    "\n",
    "The eigendecomposition can be calculated in NumPy using the eig() function.\n",
    "\n",
    "The example below first defines a 3×3 square matrix. The eigendecomposition is calculated on the matrix returning the eigenvalues and eigenvectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-20T00:29:10.074115Z",
     "start_time": "2020-04-20T00:29:10.061153Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2 3]\n",
      " [4 5 6]\n",
      " [7 8 9]]\n",
      "[ 1.61168440e+01 -1.11684397e+00 -1.30367773e-15]\n",
      "[[-0.23197069 -0.78583024  0.40824829]\n",
      " [-0.52532209 -0.08675134 -0.81649658]\n",
      " [-0.8186735   0.61232756  0.40824829]]\n"
     ]
    }
   ],
   "source": [
    "# eigendecomposition\n",
    "from numpy import array\n",
    "from numpy.linalg import eig\n",
    "# define matrix\n",
    "A = array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
    "print(A)\n",
    "# calculate eigendecomposition\n",
    "values, vectors = eig(A)\n",
    "print(values)\n",
    "print(vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.26.3.2 Confirm an Eigenvector and Eigenvalue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-20T00:29:12.243156Z",
     "start_time": "2020-04-20T00:29:12.238173Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ -3.73863537  -8.46653421 -13.19443305]\n",
      "[ -3.73863537  -8.46653421 -13.19443305]\n"
     ]
    }
   ],
   "source": [
    "# confirm eigenvector\n",
    "from numpy import array\n",
    "from numpy.linalg import eig\n",
    "# define matrix\n",
    "A = array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
    "# calculate eigendecomposition\n",
    "values, vectors = eig(A)\n",
    "# confirm first eigenvector\n",
    "B = A.dot(vectors[:, 0])\n",
    "print(B)\n",
    "C = vectors[:, 0] * values[0]\n",
    "print(C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.26.3.3 Reconstruct Original Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can reverse the process and reconstruct the original matrix given only the eigenvectors and eigenvalues.\n",
    "\n",
    "First, the list of eigenvectors must be converted into a matrix, where each vector becomes a row. The eigenvalues need to be arranged into a diagonal matrix. The NumPy diag() function can be used for this.\n",
    "\n",
    "Next, we need to calculate the inverse of the eigenvector matrix, which we can achieve with the inv() NumPy function. Finally, these elements need to be multiplied together with the dot() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-20T00:29:20.063503Z",
     "start_time": "2020-04-20T00:29:20.054527Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2 3]\n",
      " [4 5 6]\n",
      " [7 8 9]]\n",
      "[[1. 2. 3.]\n",
      " [4. 5. 6.]\n",
      " [7. 8. 9.]]\n"
     ]
    }
   ],
   "source": [
    "# reconstruct matrix\n",
    "from numpy import diag\n",
    "from numpy import dot\n",
    "from numpy.linalg import inv\n",
    "from numpy import array\n",
    "from numpy.linalg import eig\n",
    "# define matrix\n",
    "A = array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
    "print(A)\n",
    "# calculate eigenvectors and eigenvalues\n",
    "values, vectors = eig(A)\n",
    "# create matrix from eigenvectors\n",
    "Q = vectors\n",
    "# create inverse of eigenvectors matrix\n",
    "R = inv(Q)\n",
    "# create diagonal matrix from eigenvalues\n",
    "L = diag(values)\n",
    "# reconstruct the original matrix\n",
    "B = Q.dot(L).dot(R)\n",
    "print(B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.27 How to find eigen values of a matrix?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.28 Write the eigendecomposition formula for a matrix. If the matrix is real symmetric, how will this change?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is not always possible to find enough linearly independent eigenvectors for the above process to work. For instance the matrix\n",
    "\n",
    "A= [[1 0]\n",
    "    [1 1]]\n",
    " \n",
    "has only a single eigenvector, namely  (0,1) . To handle such matrices, we require more advanced techniques than we can cover (such as the Jordan Normal Form, or Singular Value Decomposition). We will often need to restrict our attention to those matrices where we can guarantee the existence of a full set of eigenvectors.\n",
    "\n",
    "The most commonly encountered family are the symmetric matrices, which are those matrices where  $A=A^{⊤}$ . In this case, we may take  W  to be an orthogonal matrix—a matrix whose columns are all length one vectors that are at right angles to one another, where  $W^{⊤}$=$W^{-1}$ —and all the eigenvalues will be real.\n",
    "Thus, in this special case, we can write as :\n",
    "\n",
    "$A=WΣW^{⊤}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.29 Is the Eigendecomposition guaranteed to be unique? If not, then how do we represent it?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The eigenvalues of a matrix A are defined as the set of values λ for which the matrix A−λI is singular. Put another way, the eigenvalues of the matrix A are the set of values λ for which p(λ)=det(A−λI)=0\n",
    "\n",
    "The expression : $det(A−λI)$ <br/>\n",
    "\n",
    "is called the characteristic polynomial of A and the eigenvalues are defined to be the roots of this polynomial. <br/>\n",
    "\n",
    "In general, the characteristic polynomial of an n×n matrix is an $N^{th}$ degree polynomial which means that there will be (at most) roots of the polynomial. The set of eigenvalues is what we call the spectrum of. The spectrum is the set of values which appears on the diagonal of your diagonal matrix. These values are unique but only up to order.\n",
    "So let me now address your question: \"Are the eigenvalues of a matrix unique?\" Well that's a bit difficult to answer because the question is not formulated well. If a matrix has distinct eigenvalues, would you consider each eigenvalue to be unique (in the sense of multiplicity one)? If a matrix has only a single eigenvalue of multiplicity, would you consider that to be unique?\n",
    "In either case, the answer to your question would be no. A matrix does not necessarily have distinct eigenvalues (although almost all do), and a matrix does not necessarily have a single eigenvalue with multipicity. In fact, given any set of values, you can construct a matrix with those values as eigenvalues (indeed just take the corresponding diagonal matrix).\n",
    "Now onto eigenvectors. For each eigenvalue λ, there exists a subspace of vectors $E_λ$ which satisfies the equation : $Av=λv$ for $v$  ∈ $E_λ$. \n",
    "Now this eigenspace $E_λ$ is unique, but the vectors in the space, the eigenvectors are not unique. It is analogous to the fact that you can talk about there being a unique x-axis, but it makes no sense to talk about a unique point on the x-axis. \n",
    "What is true is that the eigenspaces of different eigenvalues are independent, so that eigenvectors of different eigenvalues are linearly independent. When your matrix is diagonalizable, the collection (or direct sum if you are familiar with the term) of these eigenspaces is your entire vector space. This means that there exists a basis of solely eigenvectors and your matrix $S$ is formed from a basis of eigenvectors as its columns. Of course, each eigenspace is in fact a subspace, so linear combinations of eigenvectors remain eigenectors. This is why you can multiply your eigenvectors by scalar multiples and still have them remain eigenvectors. In fact, you are free to choose any basis for the eigenspace and your matrix $S$ will correspondingly be modified."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.30 What are positive definite, negative definite, positive semi definite and negative semi definite matrices?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.31 What is Singular Value Decomposition? Why do we use it? Why not just use ED?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[SVD applications](http://www.columbia.edu/itc/applied/e3101/SVD_applications.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the eigendecomposition $A=PDP^{−1}$ and SVD $A=UΣV∗$. Some key differences are as follows,\n",
    "\n",
    "\n",
    "The vectors in the eigendecomposition matrix P are not necessarily orthogonal, so the change of basis isn't a simple rotation. On the other hand, the vectors in the matrices U and V in the SVD are orthonormal, so they do represent rotations (and possibly flips).\n",
    "\n",
    "In the SVD, the nondiagonal matrices $U$ and $V$ are not necessairily the inverse of one another. They are usually not related to each other at all. In the eigendecomposition the nondiagonal matrices $P$ and $P^{−1}$ are inverses of each other.\n",
    "\n",
    "In the SVD the entries in the diagonal matrix Σ are all real and nonnegative. In the eigendecomposition, the entries of D can be any complex number - negative, positive, imaginary, whatever.\n",
    "\n",
    "The SVD always exists for any sort of rectangular or square matrix, whereas the eigendecomposition can only exists for square matrices, and even among square matrices sometimes it doesn't exist."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.32 Given a matrix A, how will you calculate its Singular Value Decomposition?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[singular value decomposition for machine learning](https://machinelearningmastery.com/singular-value-decomposition-for-machine-learning/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.33 What are singular values, left singulars and right singulars?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The diagonal values in the Sigma matrix are known as the singular values of the original matrix $A$.\n",
    "\n",
    "The columns of $V$ in the singular value decomposition, called the right singular vectors of $A$, always form an orthogonal set with no assumptions on $A$.\n",
    "\n",
    "The columns of $U$ are called the left singular vectors and they also form an orthogonal set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[What is SVD and why is it useful in machine learning](https://www.quora.com/What-is-SVD-and-why-is-it-useful-in-Machine-Learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.34 Why are singular values always non-negative?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " the positivity of the singular values is purely conventional."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[why are singular values always non negative](https://math.stackexchange.com/questions/2060572/why-are-singular-values-always-non-negative)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.36 What is the Moore Penrose pseudo inverse and how to calculate it?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.37 If we do Moore Penrose pseudo inverse on Ax = b, what solution is provided is A is fat? Moreover, what solution is provided if A is tall?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.38 Which matrices can be decomposed by ED?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only diagonalizable matrices can be factorized in this way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.39 Which matrices can be decomposed by SVD?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The singular value decomposition is very general in the sense that it can be applied to any **m × n** matrix, whereas eigenvalue decomposition can only be applied to diagonalizable matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.40 What is the trace of a matrix?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In linear algebra, the trace (often abbreviated to tr) of a square matrix $A$ is defined to be the sum of elements on the main diagonal (from the upper left to the lower right) of $A$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.41 How to write Frobenius norm of a matrix A in terms of trace?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.42 Why is trace of a multiplication of matrices invariant to cyclic permutations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.43 What is the trace of a scalar?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.44 Write the frobenius norm of a matrix in terms of trace?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 : Numerical Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Wiki github.io -- Deep Learning, Computation & optimization](https://jhui.github.io/2017/01/05/Deep-learning-computation-and-optimization/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[course on numerical optimization](https://www.cs.toronto.edu/~hinton/csc2515/notes/lec6tutorial.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 What is underflow and overflow?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overflow is when the absolute value of the number is too high for the computer to represent it.\n",
    "\n",
    "Underflow is when the absolute value of the number is too close to zero for the computer to represent it.\n",
    "\n",
    "You can get overflow with both integers and floating point numbers. You can only get underflow with floating point numbers.\n",
    "\n",
    "To get an overflow, repeatedly multiply a number by ten. To get an underflow repeatedly divide it by ten.\n",
    "\n",
    "If the variable x is a signed byte it can have values in the range -128 to +127, then\n",
    "\n",
    "x = 127 <br/>\n",
    "x = x + 1 <br/>\n",
    "will result in an overflow. +128 is not a valid value for x.\n",
    "\n",
    "For floating point numbers, the range depends on their representation. If x is a single precision (32-bit IEEE) number, then\n",
    "\n",
    "x = 1e-38 <br/>\n",
    "x = x / 1000 <br/>\n",
    "will result in an underflow. 1e-42 is not a valid value for x."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 How to tackle the problem of underflow or overflow for softmax function or log softmax function?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overflow: It occurs when very large numbers are approximated as infinity\n",
    "\n",
    "Underflow: It occurs when very small numbers (near zero in the number line) are approximated (i.e. rounded to) as zero\n",
    "\n",
    "To combat these issues when doing softmax computation, a common trick is to shift the input vector by subtracting the maximum element in it from all elements. For the input vector x, define z such that:\n",
    "\n",
    "$z = x-max(x)$ <br/>\n",
    "And then take the softmax of the new (stable) vector z\n",
    "\n",
    "**Example:** <br/>\n",
    "\n",
    "**In [266]:** def stable_softmax(x):<br/>\n",
    "....................z = x - max(x)<br/>\n",
    "....................numerator = np.exp(z)<br/>\n",
    "....................denominator = np.sum(numerator)<br/>\n",
    "....................softmax = numerator/denominator<br/>\n",
    "....................return softmax<br/>\n",
    "\n",
    "\n",
    "**In [267]:** vec = np.array([1, 2, 3, 4, 5])<br/>\n",
    "\n",
    "**In [268]:** stable_softmax(vec)<br/>\n",
    "**Out[268]:** array([ 0.01165623,  0.03168492,  0.08612854,  0.23412166,  0.63640865])<br/>\n",
    "\n",
    "**In [269]:** vec = np.array([12345, 67890, 99999999])<br/>\n",
    "\n",
    "**In [270]:** stable_softmax(vec)<br/>\n",
    "**Out[270]:** array([ 0.,  0.,  1.])<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 What is poor conditioning?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conditioning measures how rapidly the output changed with tiny changes in input.<br/>\n",
    "Poorly conditioned matrix $A$ is a matrix with a high condition number. $A^{−1}$ amplifies input errors. Small errors in x can change the output of $A^{−1}x$ rapidly ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 What is the condition number?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[condition number](http://www.seas.ucla.edu/~vandenbe/133A/lectures/condition.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[How to computer condition number of a matrix in python](https://www.w3resource.com/python-exercises/numpy/linear-algebra/numpy-linear-algebra-exercise-9.php)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Condition number**<br/>\n",
    "**Definition**: the condition number of a nonsingular matrix A is : <br/>\n",
    "\n",
    "$κ(A) = ||A||_2.||A^{−1}||_2$<br/>\n",
    "\n",
    "**Properties**<br/>\n",
    "• $κ(A) ≥ 1$ for all $A$ <br/>\n",
    "• $A$ is a well-conditioned matrix if $κ(A)$ is small (close to 1):<br/>\n",
    "the relative error in x is not much larger than the relative error in b<br/>\n",
    "• $A$ is badly conditioned or ill-conditioned if $κ(A)$ is large:<br/>\n",
    "the relative error in x can be much larger than the relative error in b <br/>\n",
    "\n",
    "From Wikipedia, In the field of numerical analysis, the condition number of a function with respect to an argument measures how much the output value of the function can change for a small change in the input argument. This is used to measure how sensitive a function is to changes or errors in the input, and how much error in the output results from an error in the input. Very frequently, one is solving the inverse problem – given $f(x)=y, f(x) = y$, one is solving for $x$, and thus the condition number of the (local) inverse must be used. In linear regression the condition number can be used as a diagnostic for multicollinearity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 What are grad, div and curl?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Grad** : The gradient of a scalar field is a vector field and whose magnitude is the rate of change and which points in the direction of the greatest rate of increase of the scalar field. <br/>\n",
    "\n",
    "**Div** :   the divergence of a vector field. <br/>\n",
    "\n",
    "**Curl** : The curl vector is the normal around which the greatest circulation exists for that point in the vector field. <br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 What are critical or stationary points in multi-dimensions?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All stationary points are critical points but not all critical points are stationary points.\n",
    "A more accurate definition of the two: <br/>\n",
    "**Critical Point:** <br/>\n",
    "Let $f$ be defined at $c$.\n",
    "\n",
    "Then, we have critical point wherever $f′(c)=0$ or wherever $f(c)$ is not differentiable (or equivalently, $f′(c)$ is not defined). Points where $f′(c)$ is not defined are called singular points and points where $f′(c)$ is 0 are called stationary points. <br/>\n",
    "[stackoverflow conversation](https://math.stackexchange.com/questions/1368188/what-is-the-difference-between-stationary-point-and-critical-point-in-calculus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7 Why should you do gradient descent when you want to minimize a function?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are standard  solvers which could be used instead of gradient descent (and often are). If the number of data points is very hight, using a standard least squares solver might be too expensive, and (stochastic) gradient descent might give you a solution that is as good in terms of test-set error as a more precise solution, with a run-time that is orders of magnitude smaller (see this great chapter by Leon Bottou)\n",
    "If your problem is small that it can be efficiently solved by an off-the-shelf least squares solver, you should probably not do gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.8 What is line search?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The line search approach first finds a descent direction along which the objective function $f$ will be reduced and then computes a step size that determines how far $x$ should move along that direction. The descent direction can be computed by various methods, such as gradient descent, Newton's method and quasi-Newton method. The step size can be determined either exactly or inexactly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.9 What is hill climbing?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hill climbing is an optimization technique that is used to find a \"local optimum\" solution to a computational problem. It starts off with a solution that is very poor compared to the optimal solution and then iteratively improves from there. It does this by generating \"neighbor\" solutions which are relatively a step better than the current solution, picks the best and then repeats the process until it arrives at the most optimal solution because it can no longer find any improvements. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.10 What is a Jacobian matrix?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[course on jacobians](http://www.stat.rice.edu/~dobelman/notes_papers/math/Jacobian.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " A Jacobian Matrix can be defined as a matrix that contains a first-order partial derivative for a vector function. The Jacobian Matrix can be of any form. It can be a rectangular matrix, where the number of rows and columns are not the same, or it can be a square matrix, where the number of rows and columns are equal. Let’s try to represent a Jacobian Matrix in a more rigorous mathematical sense.<br/>\n",
    "\n",
    "The $f: ℝn → ℝm$ is a function that takes as input the vector x ∈ ℝn and produces as output the vector $f(x) ∈ ℝm$. Then, the Jacobian matrix $J$ of $f$ is an **m×n** matrix, usually defined and arranged as follows: <br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-20T01:41:15.376660Z",
     "start_time": "2020-04-20T01:41:15.358708Z"
    }
   },
   "source": [
    "![Image of jacobian](https://www.scienceabc.com/wp-content/uploads/2019/02/spheircal.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The entry for this matrix is usually a variable like x. Knowing this is highly imperative, as this indicates that the function is differentiable at the point x. Being differentiable at a point indicates that the matrix can be mapped and given a geometric and visual approach to understanding the equations at hand. The most important kinds of Jacobian Matrix are the Polar-Cartesian and Spherical-Cartesian. These matrices are extremely important, as they help in the conversion of one coordinate system into another, which proves to be useful in many mathematical and scientific endeavors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-20T01:42:43.845176Z",
     "start_time": "2020-04-20T01:42:43.818219Z"
    }
   },
   "source": [
    "![image of jacobian](https://www.scienceabc.com/wp-content/uploads/2019/02/polar.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The importance of the Jacobian Matrix is critical in all fields of mathematics, science and engineering. One prime example is in the field of control engineering, where the use of Jacobian matrices allows the local (approximate) linearization of non-linear systems around a given equilibrium point, thus allowing the use of linear systems techniques, such as the calculation of eigenvalues (and thus allowing an indication of the type of the equilibrium point). Jacobian matrices are also used in the estimation of the internal states of non-linear systems in the construction of an extended Kalman filter. Basically, we can conclude by saying that Jacobian matrices maintain a truly unique and important place in the world of matrices!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.11 What is curvature?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "• The local geometry of curvature is measured by the Hessian: the\n",
    "matrix of second-order partial derivatives: $H_ij = ∂^{2}E/∂_w{_i}w{_j}$ <br/>\n",
    "• Eigenvectors/eigenvalues of the Hessian describe the directions of\n",
    "principal curvature and the amount of curvature in each direction. <br/>\n",
    "• Maximum sensible stepsize is $2/λ_{max}$ <br/>\n",
    "• Rate of convergence depends on $(1 − 2(λ_{min}/λ_{max}))$ <br/>\n",
    "\n",
    "*refer to course on optimization at the beginning of this part 2*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.12 What is a Hessian matrix?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Hessian matrix has a number of\n",
    "important applications in a variety of\n",
    "different fields, such as optimzation,\n",
    "image processing and statistics. Geometrically, the Hessian matrix describes the local curvature of scalar\n",
    "functions $f : R^{P} → R$, and is for this\n",
    "reason perhaps mostly known in the\n",
    "field of optimization. Nevertheless,\n",
    "the Hessian matrix also has an important role in statistics, since its inverse\n",
    "is related to the powerful concept of\n",
    "uncertainty approximation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3 : Basics of Probability and Information Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Compare “Frequentist probability” vs. “Bayesian probability”?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Bayesian school models uncertainty by a probability distribution over hypotheses.\n",
    "One’s ability to make inferences depends on one’s degree of confidence in the chosen prior,\n",
    "and the robustness of the findings to alternate prior distributions may be relevant and\n",
    "important.<br/>\n",
    "The frequentist school only uses conditional distributions of data given specific hypotheses.\n",
    "The presumption is that some hypothesis (parameter specifying the conditional distribution\n",
    "of the data) is true and that the observed data is sampled from that distribution. In\n",
    "particular, the frequentist approach does not depend on a subjective prior that may vary\n",
    "from one investigator to another. <br/>\n",
    "These two schools may be further contrasted as follows:<br/>\n",
    "<br/>\n",
    "Bayesian inference<br/>\n",
    "• uses probabilities for both hypotheses and data.<br/>\n",
    "• depends on the prior and likelihood of observed data.<br/>\n",
    "• requires one to know or construct a ‘subjective prior’.<br/>\n",
    "• dominated statistical practice before the 20th century.<br/>\n",
    "• may be computationally intensive due to integration over many parameters.<br/>\n",
    "<br/>\n",
    "Frequentist inference (NHST)<br/>\n",
    "• never uses or gives the probability of a hypothesis (no prior or posterior).<br/>\n",
    "• depends on the likelihood P(D | H)) for both observed and unobserved data.<br/>\n",
    "• does not require a prior.<br/>\n",
    "• dominated statistical practice during the 20th century.<br/>\n",
    "• tends to be less computationally intensive.<br/>\n",
    "Frequentist measures like p-values and confidence intervals continue to dominate research,\n",
    "especially in the life sciences. However, in the current era of powerful computers and\n",
    "big data, Bayesian methods have undergone an enormous renaissance in fields like machine learning and genetics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[introduction to probability and statistics - MIT](https://ocw.mit.edu/courses/mathematics/18-05-introduction-to-probability-and-statistics-spring-2014/readings/MIT18_05S14_Reading20.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 What is a random variable?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Medium : what is a random variable ?](https://towardsdatascience.com/but-what-is-a-random-variable-4265d84cb7e5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 What is a probability distribution?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A probability distribution is a list of outcomes and their associated probabilities.<br/>\n",
    "We can write small distributions with tables but it’s easier to summarise large distributions with functions. <br/>\n",
    "A function that represents a discrete probability distribution is called a probability mass function.<br/>\n",
    "A function that represents a continuous probability distribution is called a probability density function.<br/>\n",
    "Functions that represent probability distributions still have to obey the rules of probability<br/>\n",
    "The output of a probability mass function is a probability whereas the area under the curve produced by a probability density function represents a probability.<br/>\n",
    "Parameters of a probability function play a central role in defining the probabilities of the outcomes of a random variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 What is a probability mass function?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Probability Mass Function is also termed as a frequency function and is a vital part of statistics. Probability Mass Function integrates that any given variable has the probability that the random number will be equal to that variable. All the probabilities for the given discrete random variables provided by Probability Mass Function. Here discrete essentially means that there are a set number of outcomes for the variables. For understanding discrete variables better, the set number of outcomes in a die can only be 1, 2, 3, 4, 5 or 6. Here a discrete random value when considering a die is a set of random variables which are finite."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 What is a probability density function?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a continuous function, the probability density function (pdf) is the probability that the variate has the value x. Since for continuous distributions the probability at a single point is zero, this is often expressed in terms of an integral between two points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6 What is a joint probability distribution?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.7 What are the conditions for a function to be a probability mass function?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.8 What are the conditions for a function to be a probability density function?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.9 What is a marginal probability? Given the joint probability function, how will you calculate it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.10 What is conditional probability? Given the joint probability function, how will you calculate it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.11 State the Chain rule of conditional probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.12 What are the conditions for independence and conditional independence of two random variables?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.13 What are expectation, variance and covariance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.14 Compare covariance and independence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.15 What is the covariance for a vector of random variables?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.16 What is a Bernoulli distribution? Calculate the expectation and variance of a random variable that follows Bernoulli distribution?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.17 What is a multinoulli distribution?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.18 What is a normal distribution?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.19 Why is the normal distribution a default choice for a prior over a set of real numbers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.20 What is the central limit theorem?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.21 What are exponential and Laplace distribution?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.22 What are Dirac distribution and Empirical distribution?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.23 What is mixture of distributions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.24 Name two common examples of mixture of distributions? (Empirical and Gaussian Mixture)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.25 Is Gaussian mixture model a universal approximator of densities?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.26 Write the formulae for logistic and softplus function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.27 Write the formulae for Bayes rule.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.28 What do you mean by measure zero and almost everywhere?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.29 If two random variables are related in a deterministic way, how are the PDFs related?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.30 Define self-information. What are its units?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.31 What are Shannon entropy and differential entropy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.32 What is Kullback-Leibler (KL) divergence?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.33 Can KL divergence be used as a distance measure?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.34 Define cross-entropy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.35 What are structured probabilistic models or graphical models?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.36 In the context of structured probabilistic models, what are directed and undirected models? How are they represented? What are cliques in undirected structured probabilistic models?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4 : Confidence interval\n",
    "### 4.1 What is population mean and sample mean?\n",
    "### 4.2 What is population standard deviation and sample standard deviation?\n",
    "### 4.3 Why population s.d. has N degrees of freedom while sample s.d. has N-1 degrees of freedom? In other words, why 1/N inside root for pop. s.d. and 1/(N-1) inside root for sample s.d.?\n",
    "### 4.4 What is the formula for calculating the s.d. of the sample mean?\n",
    "### 4.5 What is confidence interval?\n",
    "### 4.6 What is standard error?\n",
    "\n",
    "\n",
    "## Part 5 : Learning Theory\n",
    "### 5.1 Describe bias and variance with examples.\n",
    "### 5.2 What is Empirical Risk Minimization?\n",
    "### 5.3 What is Union bound and Hoeffding’s inequality?\n",
    "### 5.4 Write the formulae for training error and generalization error. Point out the differences.\n",
    "### 5.5 State the uniform convergence theorem and derive it.\n",
    "### 5.6 What is sample complexity bound of uniform convergence theorem?\n",
    "### 5.7 What is error bound of uniform convergence theorem?\n",
    "### 5.8 What is the bias-variance trade-off theorem?\n",
    "### 5.9 From the bias-variance trade-off, can you derive the bound on training set size?\n",
    "### 5.10 What is the VC dimension?\n",
    "### 5.11 What does the training set size depend on for a finite and infinite hypothesis set? Compare and contrast.\n",
    "### 5.12 What is the VC dimension for an n-dimensional linear classifier?\n",
    "### 5.13 How is the VC dimension of a SVM bounded although it is projected to an infinite dimension?\n",
    "### 5.14 Considering that Empirical Risk Minimization is a NP-hard problem, how does logistic regression and SVM loss work?\n",
    "\n",
    "\n",
    "## Part 6 : Model and feature selection\n",
    "### 6.1 Why are model selection methods needed?\n",
    "### 6.2 How do you do a trade-off between bias and variance?\n",
    "### 6.3 What are the different attributes that can be selected by model selection methods?\n",
    "### 6.4 Why is cross-validation required?\n",
    "### 6.5 Describe different cross-validation techniques.\n",
    "### 6.6 What is hold-out cross validation? What are its advantages and disadvantages?\n",
    "### 6.7 What is k-fold cross validation? What are its advantages and disadvantages?\n",
    "### 6.8 What is leave-one-out cross validation? What are its advantages and disadvantages?\n",
    "### 6.9 Why is feature selection required?\n",
    "### 6.10 Describe some feature selection methods.\n",
    "### 6.11 What is forward feature selection method? What are its advantages and disadvantages?\n",
    "### 6.12 What is backward feature selection method? What are its advantages and disadvantages?\n",
    "### 6.13 What is filter feature selection method and describe two of them?\n",
    "### 6.14 What is mutual information and KL divergence?\n",
    "### 6.15 Describe KL divergence intuitively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Curse of dimensionality\n",
    "Describe the curse of dimensionality with examples.\n",
    "What is local constancy or smoothness prior or regularization?\n",
    "Universal approximation of neural networks\n",
    "State the universal approximation theorem? What is the technique used to prove that?\n",
    "What is a Borel measurable function?\n",
    "Given the universal approximation theorem, why can’t a Multi Layer Perceptron (MLP) still reach an arbitrarily small positive error?\n",
    "Deep Learning motivation\n",
    "What is the mathematical motivation of Deep Learning as opposed to standard Machine Learning techniques?\n",
    "In standard Machine Learning vs. Deep Learning, how is the order of number of samples related to the order of regions that can be recognized in the function space?\n",
    "What are the reasons for choosing a deep model as opposed to shallow model?\n",
    "How Deep Learning tackles the curse of dimensionality?\n",
    "Support Vector Machine\n",
    "How can the SVM optimization function be derived from the logistic regression optimization function?\n",
    "What is a large margin classifier?\n",
    "Why SVM is an example of a large margin classifier?\n",
    "SVM being a large margin classifier, is it influenced by outliers?\n",
    "What is the role of C in SVM?\n",
    "In SVM, what is the angle between the decision boundary and theta?\n",
    "What is the mathematical intuition of a large margin classifier?\n",
    "What is a kernel in SVM? Why do we use kernels in SVM?\n",
    "What is a similarity function in SVM? Why it is named so?\n",
    "How are the landmarks initially chosen in an SVM? How many and where?\n",
    "Can we apply the kernel trick to logistic regression? Why is it not used in practice then?\n",
    "What is the difference between logistic regression and SVM without a kernel?\n",
    "How does the SVM parameter C affect the bias/variance trade off?\n",
    "How does the SVM kernel parameter sigma² affect the bias/variance trade off?\n",
    "Can any similarity function be used for SVM?\n",
    "Logistic regression vs. SVMs: When to use which one?\n",
    "Bayesian Machine Learning\n",
    "What are the differences between “Bayesian” and “Freqentist” approach for Machine Learning?\n",
    "Compare and contrast maximum likelihood and maximum a posteriori estimation.\n",
    "How does Bayesian methods do automatic feature selection?\n",
    "What do you mean by Bayesian regularization?\n",
    "When will you use Bayesian methods instead of Frequentist methods?\n",
    "Regularization\n",
    "What is L1 regularization?\n",
    "What is L2 regularization?\n",
    "Compare L1 and L2 regularization.\n",
    "Why does L1 regularization result in sparse models?\n",
    "What is dropout?\n",
    "How will you implement dropout during forward and backward pass?\n",
    "Evaluation of Machine Learning systems\n",
    "What are accuracy, sensitivity, specificity, ROC?\n",
    "What are precision and recall?\n",
    "Describe t-test in the context of Machine Learning.\n",
    "Clustering\n",
    "Describe the k-means algorithm.\n",
    "What is distortion function? Is it convex or non-convex?\n",
    "Tell me about the convergence of the distortion function.\n",
    "Topic: EM algorithm\n",
    "What is the Gaussian Mixture Model?\n",
    "Describe the EM algorithm intuitively.\n",
    "What are the two steps of the EM algorithm\n",
    "Compare Gaussian Mixture Model and Gaussian Discriminant Analysis.\n",
    "Dimensionality Reduction\n",
    "Why do we need dimensionality reduction techniques?\n",
    "What do we need PCA and what does it do?\n",
    "What is the difference between logistic regression and PCA?\n",
    "What are the two pre-processing steps that should be applied before doing PCA?\n",
    "Basics of Natural Language Processing\n",
    "What is WORD2VEC?\n",
    "What is t-SNE? Why do we use PCA instead of t-SNE?\n",
    "What is sampled softmax?\n",
    "Why is it difficult to train a RNN with SGD?\n",
    "How do you tackle the problem of exploding gradients?\n",
    "What is the problem of vanishing gradients?\n",
    "How do you tackle the problem of vanishing gradients?\n",
    "Explain the memory cell of a LSTM.\n",
    "What type of regularization do one use in LSTM?\n",
    "What is Beam Search?\n",
    "How to automatically caption an image?\n",
    "Some basic questions\n",
    "Can you state Tom Mitchell’s definition of learning and discuss T, P and E?\n",
    "What can be different types of tasks encountered in Machine Learning?\n",
    "What are supervised, unsupervised, semi-supervised, self-supervised, multi-instance learning, and reinforcement learning?\n",
    "Loosely how can supervised learning be converted into unsupervised learning and vice-versa?\n",
    "Consider linear regression. What are T, P and E?\n",
    "Derive the normal equation for linear regression.\n",
    "What do you mean by affine transformation? Discuss affine vs. linear transformation.\n",
    "Discuss training error, test error, generalization error, overfitting, and underfitting.\n",
    "Compare representational capacity vs. effective capacity of a model.\n",
    "Discuss VC dimension.\n",
    "What are nonparametric models? What is nonparametric learning?\n",
    "What is an ideal model? What is Bayes error? What is/are the source(s) of Bayes error occur?\n",
    "What is the no free lunch theorem in connection to Machine Learning?\n",
    "What is regularization? Intuitively, what does regularization do during the optimization procedure?\n",
    "What is weight decay? What is it added?\n",
    "What is a hyperparameter? How do you choose which settings are going to be hyperparameters and which are going to be learned?\n",
    "Why is a validation set necessary?\n",
    "What are the different types of cross-validation? When do you use which one?\n",
    "What are point estimation and function estimation in the context of Machine Learning? What is the relation between them?\n",
    "What is the maximal likelihood of a parameter vector $theta$? Where does the log come from?\n",
    "Prove that for linear regression MSE can be derived from maximal likelihood by proper assumptions.\n",
    "Why is maximal likelihood the preferred estimator in ML?\n",
    "Under what conditions do the maximal likelihood estimator guarantee consistency?\n",
    "What is cross-entropy of loss?\n",
    "What is the difference between loss function, cost function and objective function?\n",
    "Optimization procedures\n",
    "What is the difference between an optimization problem and a Machine Learning problem?\n",
    "How can a learning problem be converted into an optimization problem?\n",
    "What is empirical risk minimization? Why the term empirical? Why do we rarely use it in the context of deep learning?\n",
    "Name some typical loss functions used for regression. Compare and contrast.\n",
    "What is the 0–1 loss function? Why can’t the 0–1 loss function or classification error be used as a loss function for optimizing a deep neural network?\n",
    "Sequence Modeling\n",
    "Write the equation describing a dynamical system. Can you unfold it? Now, can you use this to describe a RNN?\n",
    "What determines the size of an unfolded graph?\n",
    "What are the advantages of an unfolded graph?\n",
    "What does the output of the hidden layer of a RNN at any arbitrary time t represent?\n",
    "Are the output of hidden layers of RNNs lossless? If not, why?\n",
    "RNNs are used for various tasks. From a RNNs point of view, what tasks are more demanding than others?\n",
    "Discuss some examples of important design patterns of classical RNNs.\n",
    "Write the equations for a classical RNN where hidden layer has recurrence. How would you define the loss in this case? What problems you might face while training it?\n",
    "What is backpropagation through time?\n",
    "Consider a RNN that has only output to hidden layer recurrence. What are its advantages or disadvantages compared to a RNN having only hidden to hidden recurrence?\n",
    "What is Teacher forcing? Compare and contrast with BPTT.\n",
    "What is the disadvantage of using a strict teacher forcing technique? How to solve this?\n",
    "Explain the vanishing/exploding gradient phenomenon for recurrent neural networks.\n",
    "Why don’t we see the vanishing/exploding gradient phenomenon in feedforward networks?\n",
    "What is the key difference in architecture of LSTMs/GRUs compared to traditional RNNs?\n",
    "What is the difference between LSTM and GRU?\n",
    "Explain Gradient Clipping.\n",
    "Adam and RMSProp adjust the size of gradients based on previously seen gradients. Do they inherently perform gradient clipping? If no, why?\n",
    "Discuss RNNs in the context of Bayesian Machine Learning.\n",
    "Can we do Batch Normalization in RNNs? If not, what is the alternative?\n",
    "Autoencoders\n",
    "What is an Autoencoder? What does it “auto-encode”?\n",
    "What were Autoencoders traditionally used for? Why there has been a resurgence of Autoencoders for generative modeling?\n",
    "What is recirculation?\n",
    "What loss functions are used for Autoencoders?\n",
    "What is a linear autoencoder? Can it be optimal (lowest training reconstruction error)? If yes, under what conditions?\n",
    "What is the difference between Autoencoders and PCA?\n",
    "What is the impact of the size of the hidden layer in Autoencoders?\n",
    "What is an undercomplete Autoencoder? Why is it typically used for?\n",
    "What is a linear Autoencoder? Discuss it’s equivalence with PCA. Which one is better in reconstruction?\n",
    "What problems might a nonlinear undercomplete Autoencoder face?\n",
    "What are overcomplete Autoencoders? What problems might they face? Does the scenario change for linear overcomplete autoencoders?\n",
    "Discuss the importance of regularization in the context of Autoencoders.\n",
    "Why does generative autoencoders not require regularization?\n",
    "What are sparse autoencoders?\n",
    "What is a denoising autoencoder? What are its advantages? How does it solve the overcomplete problem?\n",
    "What is score matching? Discuss it’s connections to DAEs.\n",
    "Are there any connections between Autoencoders and RBMs?\n",
    "What is manifold learning? How are denoising and contractive autoencoders equipped to do manifold learning?\n",
    "What is a contractive autoencoder? Discuss its advantages. How does it solve the overcomplete problem?\n",
    "Why is a contractive autoencoder named so?\n",
    "What are the practical issues with CAEs? How to tackle them?\n",
    "What is a stacked autoencoder? What is a deep autoencoder? Compare and contrast.\n",
    "Compare the reconstruction quality of a deep autoencoder vs. PCA.\n",
    "What is predictive sparse decomposition?\n",
    "Discuss some applications of Autoencoders.\n",
    "Representation Learning\n",
    "What is representation learning? Why is it useful?\n",
    "What is the relation between Representation Learning and Deep Learning?\n",
    "What is one-shot and zero-shot learning (Google’s NMT)? Give examples.\n",
    "What trade offs does representation learning have to consider?\n",
    "What is greedy layer-wise unsupervised pretraining (GLUP)? Why greedy? Why layer-wise? Why unsupervised? Why pretraining?\n",
    "What were/are the purposes of the above technique? (deep learning problem and initialization)\n",
    "Why does unsupervised pretraining work?\n",
    "When does unsupervised training work? Under which circumstances?\n",
    "Why might unsupervised pretraining act as a regularizer?\n",
    "What is the disadvantage of unsupervised pretraining compared to other forms of unsupervised learning?\n",
    "How do you control the regularizing effect of unsupervised pretraining?\n",
    "How to select the hyperparameters of each stage of GLUP?\n",
    "Monte Carlo Methods\n",
    "What are deterministic algorithms?\n",
    "What are Las vegas algorithms?\n",
    "What are deterministic approximate algorithms?\n",
    "What are Monte Carlo algorithms?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Summary",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "257.594px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
